{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import ngrams\n",
    "from math import log, exp\n",
    "from operator import itemgetter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing into sentences and removing unnecessary symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens = 30713\n",
      "Number of types = 2560\n",
      "Number of sentences = 976\n"
     ]
    }
   ],
   "source": [
    "#Opening File and tokenizing into sentences\n",
    "text_file = open('Alice_in_Wonderland.txt','r').read().decode('utf-8')\n",
    "sents_current = sent_tokenize(text_file)\n",
    "sents = []\n",
    "vocab = {}\n",
    "translate_table = {}\n",
    "for char in punctuation:\n",
    "    if char == '-':\n",
    "        translate_table[ord(char)] = u\"\\u0020\"\n",
    "    else:\n",
    "        translate_table[ord(char)] = None\n",
    "\n",
    "#Removing punctuation and making vocabulary\n",
    "for sent in sents_current:\n",
    "    sent = sent.lower()\n",
    "    sent = sent.translate(translate_table) \n",
    "    words = word_tokenize(sent)\n",
    "    msent = ['<s>']\n",
    "    for word in words:\n",
    "        if len(set(word).intersection(punctuation)) == 0 and (len(word)>1 or word == 'a' or word == 'i' or word == 'o') :\n",
    "            msent.append(word)\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    msent.append('</s>')\n",
    "    fsent = \" \".join(msent)\n",
    "    sents.append(fsent)\n",
    "n_sents = len(sents)\n",
    "vocab['<s>'] = 2*n_sents\n",
    "vocab['</s>'] = 2*n_sents\n",
    "\n",
    "tokens = sum(vocab.values())\n",
    "types = len(vocab)\n",
    "print 'Number of tokens = ' + str(tokens)\n",
    "print 'Number of types = ' + str(types)\n",
    "print 'Number of sentences = ' + str(n_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving dataset into 80% train and 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of train and test data size = 3.97959183673\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(sents, test_size=0.20, random_state=42)\n",
    "print \"Ratio of train and test data size = \" + str(len(train)*1.0/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE of Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secondly 6.51190049816e-05\n",
      "pardon 0.000195357014945\n",
      "saves 3.25595024908e-05\n",
      "knelt 3.25595024908e-05\n",
      "four 0.000260476019926\n",
      "sleep 0.000195357014945\n",
      "hanging 9.76785074724e-05\n",
      "ringlets 6.51190049816e-05\n",
      "oldest 3.25595024908e-05\n",
      "hate 6.51190049816e-05\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE\n",
    "uni_mle = {}\n",
    "for i in vocab.keys():\n",
    "    uni_mle[i] = (vocab[i]*1.0)/tokens\n",
    "    \n",
    "#Printing MLE for 10 unigrams\n",
    "for i in uni_mle.keys()[:10]:\n",
    "    print i, uni_mle[i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE of Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'at', u'ours') 0.00471698113208\n",
      "(u'fish', u'came') 0.125\n",
      "(u'you', u'have') 0.0121654501217\n",
      "(u'keep', u'herself') 0.0909090909091\n",
      "(u'with', u'great') 0.0165745856354\n",
      "(u'dodo', u'in') 0.0769230769231\n",
      "(u'hatter', u'said') 0.0357142857143\n",
      "(u'rabbit', u'was') 0.0392156862745\n",
      "(u'claws', u'and') 1.0\n",
      "(u'the', u'three') 0.00426309378806\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE\n",
    "bigrams = {}\n",
    "bi_mle = {}\n",
    "for sent in sents:\n",
    "    ssent = sent.split()\n",
    "    for i in range(0,len(ssent)-1):\n",
    "        k = (ssent[i], ssent[i+1])\n",
    "        try:\n",
    "            bigrams[k] += 1\n",
    "        except KeyError:\n",
    "            bigrams[k] = 1\n",
    "for i in bigrams.keys():\n",
    "    bi_mle[i] = (bigrams[i]*1.0)/vocab[i[0]]\n",
    "\n",
    "#Printing MLE for 10 biigrams\n",
    "for i in bi_mle.keys()[:10]:\n",
    "    print i, bi_mle[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE of Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'at', u'the', u'beginning') 0.0166666666667\n",
      "(u'a', u'friend', u'of') 1.0\n",
      "(u'alice', u'was', u'soon') 0.0588235294118\n",
      "(u'heard', u'of', u'one') 0.25\n",
      "(u'swallow', u'a', u'morsel') 1.0\n",
      "(u'at', u'it', u'uneasily') 0.142857142857\n",
      "(u'if', u'it', u'likes') 0.1\n",
      "(u'you', u'could', u'only') 0.2\n",
      "(u'house', u'opened', u'and') 1.0\n",
      "(u'she', u'remembered', u'that') 0.2\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE\n",
    "trigrams = {}\n",
    "tri_mle = {}\n",
    "for sent in sents:\n",
    "    ssent = sent.split()\n",
    "    for i in range(0,len(ssent)-2):\n",
    "        k = (ssent[i], ssent[i+1], ssent[i+2])\n",
    "        try:\n",
    "            trigrams[k] += 1\n",
    "        except KeyError:\n",
    "            trigrams[k] = 1\n",
    "for i in trigrams.keys():\n",
    "    tri_mle[i] = (trigrams[i]*1.0)/bigrams[(i[0], i[1])]\n",
    "\n",
    "#Printing MLE for 10 trigrams\n",
    "for i in tri_mle.keys()[:10]:\n",
    "    print i, tri_mle[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE of Quadrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'if', u'i', u'would', u'talk') 1.0\n",
      "(u'what', u'a', u'funny', u'watch') 1.0\n",
      "(u'hand', u'bit', u'to', u'try') 1.0\n",
      "(u'<s>', u'therefore', u'i', u'mad') 1.0\n",
      "(u'the', u'song', u'i', u'have') 1.0\n",
      "(u'me', u'hear', u'the', u'name') 1.0\n",
      "(u'which', u'word', u'sounded', u'best') 1.0\n",
      "(u'in', u'chorus', u'yes', u'please') 1.0\n",
      "(u'happens', u'and', u'if', u'i') 1.0\n",
      "(u'and', u'under', u'sentence', u'of') 1.0\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE\n",
    "quadgrams = {}\n",
    "quad_mle = {}\n",
    "for sent in sents:\n",
    "    ssent = sent.split()\n",
    "    for i in range(0,len(ssent)-3):\n",
    "        k = (ssent[i], ssent[i+1], ssent[i+2], ssent[i+3])\n",
    "        try:\n",
    "            quadgrams[k] += 1\n",
    "        except KeyError:\n",
    "            quadgrams[k] = 1\n",
    "for i in quadgrams.keys():\n",
    "    quad_mle[i] = (quadgrams[i]*1.0)/trigrams[(i[0], i[1], i[2])]\n",
    "\n",
    "#Printing MLE for 10 quadgrams\n",
    "for i in quad_mle.keys()[:10]:\n",
    "    print i, quad_mle[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of possible n-grams and number of present n-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number:13\n",
      "Number of possible 13-grams = 202824096036516704239472512860160000000000000\n",
      "Number of present 13-grams = 17941\n"
     ]
    }
   ],
   "source": [
    "def ngram(n):\n",
    "    ngram_arr = []\n",
    "    poss_ngrams = types**n\n",
    "    for sent in sents:\n",
    "        ngram_arr += list(ngrams(sent.split(), n))\n",
    "    present_ngrams = len(set(ngram_arr))\n",
    "    return poss_ngrams, present_ngrams\n",
    "\n",
    "#Printing\n",
    "n = input('Enter a number:')\n",
    "poss, present = ngram(n)\n",
    "if n == 1:\n",
    "    print \"Number of possible unigrams = \" + str(poss)\n",
    "    print \"Number of present unigrams = \" + str(present)\n",
    "elif n == 2:\n",
    "    print \"Number of possible bigrams = \" + str(poss)\n",
    "    print \"Number of present bigrams = \" + str(present)\n",
    "elif n == 3:\n",
    "    print \"Number of possible trigrams = \" + str(poss)\n",
    "    print \"Number of present trigrams = \" + str(present)\n",
    "elif n == 4:\n",
    "    print \"Number of possible quadgrams = \" + str(poss)\n",
    "    print \"Number of present quadgrams = \" + str(present)\n",
    "else:\n",
    "    print \"Number of possible \" + str(n) + \"-grams = \" + str(poss)\n",
    "    print \"Number of present \" + str(n) + \"-grams = \" + str(present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate a sentence using a given ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter value of n for n-gram:4\n",
      "<s> but at any rate i ll never go there again said alice as she stood watching them and he checked himself suddenly the others\n",
      "<s> she took down a jar from one of the legs of the table </s>\n",
      "<s> by the bye what became of the baby said the cat or you wouldn have come here alice didn think that proved it at\n",
      "<s> chapter advice from a caterpillar the caterpillar and alice looked very anxiously into its face in some alarm </s>\n",
      "<s> presently the rabbit came near her she began in a voice of thunder and people began running about in all directions tumbling up against\n",
      "<s> i didn know it was your table said alice it very interesting </s>\n",
      "<s> come that finished the guinea pigs thought alice </s>\n",
      "<s> would you like the mock turtle </s>\n"
     ]
    }
   ],
   "source": [
    "def generate(n):\n",
    "    n = int(n) #n should be an integer\n",
    "    \n",
    "    #n should not be less than one\n",
    "    if n<1:\n",
    "        print \"Error: n should be greater than or equal to one\"\n",
    "        return\n",
    "    \n",
    "    elif n>264:\n",
    "        print \"Error: too long sentences required. Corpus does not have this long sentences\"\n",
    "        return\n",
    "    #for unigram model\n",
    "    elif n == 1:\n",
    "        s = ['<s>']\n",
    "        gen_unigram = []\n",
    "        gen_prob = []\n",
    "        for i in uni_mle.keys():\n",
    "            gen_unigram.append(i)\n",
    "            gen_prob.append(uni_mle[i])\n",
    "        norm_prob = np.divide(gen_prob, sum(gen_prob))\n",
    "        while(s[-1] != '</s>' and len(s)<25):\n",
    "            a = np.random.multinomial(1, norm_prob, size=1).tolist()\n",
    "            index = a[0].index(1)\n",
    "            s.append(gen_unigram[index])\n",
    "    \n",
    "    #for ngram model where n>1\n",
    "    else:\n",
    "        #making list of ngrams and (n-1)grams\n",
    "        ngram_dict = {}\n",
    "        ngraml_dict = {}\n",
    "        for sent in sents:\n",
    "            ngram_arr_n = list(ngrams(sent.split(), n))\n",
    "            ngram_arr_nl = list(ngrams(sent.split(), n-1))\n",
    "            for i in ngram_arr_n:\n",
    "                try:\n",
    "                    ngram_dict[i] += 1\n",
    "                except KeyError:\n",
    "                    ngram_dict[i] = 1 \n",
    "            for i in ngram_arr_nl:\n",
    "                try:\n",
    "                    ngraml_dict[i] += 1\n",
    "                except KeyError:\n",
    "                    ngraml_dict[i] = 1 \n",
    "                    \n",
    "        #calculating MLE for ngram model\n",
    "        n_mle = {}\n",
    "        for i in ngram_dict.keys():\n",
    "            li = []\n",
    "            for j in range(n-1):\n",
    "                li.append(i[j])\n",
    "            n_mle[i] = (ngram_dict[i]*1.0)/ngraml_dict[tuple(li)]\n",
    "        \n",
    "        #selecting first ngram of sentence according to multinomial distribution\n",
    "        s = []\n",
    "        gen_ngram = []\n",
    "        gen_prob = []\n",
    "        for i in ngram_dict.keys():\n",
    "            if i[0] == '<s>':\n",
    "                gen_ngram.append(i)\n",
    "                gen_prob.append(n_mle[i])\n",
    "        norm_prob = np.divide(gen_prob, sum(gen_prob))\n",
    "        a = np.random.multinomial(1, norm_prob, size=1).tolist()\n",
    "        index = a[0].index(1)\n",
    "        for i in gen_ngram[index]:\n",
    "            s.append(i)\n",
    "        \n",
    "        #generating the rest of the sentence\n",
    "        while(s[-1] != '</s>' and len(s)<25):\n",
    "            gen_ngram = []\n",
    "            gen_prob = []\n",
    "            for b in ngram_dict.keys(): \n",
    "                if(list(b[:-1]) == s[-n+1:]):\n",
    "                    gen_ngram.append(b)\n",
    "                    gen_prob.append(n_mle[b])\n",
    "            norm_prob = np.divide(gen_prob, sum(gen_prob))\n",
    "            a = np.random.multinomial(1, norm_prob, size=1).tolist()\n",
    "            index = a[0].index(1)\n",
    "            s.append(gen_ngram[index][-1])\n",
    "    print ' '.join(s)\n",
    "\n",
    "n = input(\"Enter value of n for n-gram:\")\n",
    "for i in range(8):\n",
    "    generate(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of a sentence given an ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter n for ngram model:5\n",
      "Log of probability is:-0.69314718056\n",
      "Probability is:0.5\n"
     ]
    }
   ],
   "source": [
    "def probability(sentence, n):\n",
    "    #unigram model\n",
    "    if n == 1:\n",
    "        p = 0\n",
    "        for i in sentence.split():\n",
    "            p += log(uni_mle[i])\n",
    "            \n",
    "    #bigram model\n",
    "    elif n == 2:\n",
    "        p = 0\n",
    "        ssent = sentence.split()\n",
    "        for i in range(1,len(ssent)):\n",
    "            p += log(bi_mle[(ssent[i-1], ssent[i])])\n",
    "            \n",
    "    #trigram model\n",
    "    elif n == 3:\n",
    "        p=0\n",
    "        ssent = sentence.split()\n",
    "        for i in range(2,len(ssent)):\n",
    "            p += log(tri_mle[(ssent[i-2], ssent[i-1], ssent[i])])\n",
    "            \n",
    "    #quadgram model\n",
    "    elif n == 4:\n",
    "        p=0\n",
    "        ssent = sentence.split()\n",
    "        for i in range(3,len(ssent)):\n",
    "            p += log(quad_mle[(ssent[i-3], ssent[i-2], ssent[i-1], ssent[i])])\n",
    "            \n",
    "    #ngram model\n",
    "    else:\n",
    "        ngram_dict = {}\n",
    "        ngraml_dict = {}\n",
    "        p=0\n",
    "        ssent = sentence.split()\n",
    "        for sent in sents:\n",
    "            #finding ngrams and (n-1)grams with their frequencies\n",
    "            ngram_arr_n = list(ngrams(sent.split(), n))\n",
    "            ngram_arr_nl = list(ngrams(sent.split(), n-1))\n",
    "            for i in ngram_arr_n:\n",
    "                try:\n",
    "                    ngram_dict[i] += 1\n",
    "                except KeyError:\n",
    "                    ngram_dict[i] = 1 \n",
    "            for i in ngram_arr_nl:\n",
    "                try:\n",
    "                    ngraml_dict[i] += 1\n",
    "                except KeyError:\n",
    "                    ngraml_dict[i] = 1 \n",
    "                    \n",
    "        #calculating log probability\n",
    "        for i in range(n-1,len(ssent)):\n",
    "            l=[]\n",
    "            for j in range(i-n+1, i+1):\n",
    "                l.append(ssent[j])\n",
    "            t = tuple(l)\n",
    "            tl = tuple(l[:-1])\n",
    "            try:\n",
    "                p += log(ngram_dict[t]) - log(ngraml_dict[tl])\n",
    "            except:\n",
    "                p = -99999999999999999999999999999999999999999999999\n",
    "    return p\n",
    "\n",
    "sentence = '<s> stop this moment i tell you but she went on all the same shedding gallons of tears until there was a large pool all round her about four inches deep and reaching half down the hall </s>'\n",
    "n = input(\"Enter n for ngram model:\")\n",
    "prob = probability(sentence, n)\n",
    "print 'Log of probability is:' + str(prob)\n",
    "print 'Probability is:' + str(exp(prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-One smoothing for bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams, the change in count, original count, effective count after add-one smoothing for top 10 drastic changes are:\n",
      "(u'said', u'the') 176.895433488 209 32.1045665122\n",
      "(u'of', u'the') 110.594014314 133 22.4059856864\n",
      "(u'said', u'alice') 98.113170086 116 17.886829914\n",
      "(u'in', u'a') 84.6830601093 97 12.3169398907\n",
      "(u'in', u'the') 68.9453551913 79 10.0546448087\n",
      "(u'<s>', u'i') 66.5177304965 118 51.4822695035\n",
      "(u'it', u'was') 61.4786053883 76 14.5213946117\n",
      "(u'and', u'the') 60.9114219114 82 21.0885780886\n",
      "(u'at', u'the') 55.3347763348 60 4.66522366522\n",
      "(u'as', u'she') 55.22387531 61 5.77612469005\n"
     ]
    }
   ],
   "source": [
    "bia1_mle = {}\n",
    "bigramsa1 = {}\n",
    "diff = {}\n",
    "for i in bigrams.keys():\n",
    "    bia1_mle[i] = (bigrams[i] + 1.0)/(vocab[i[0]] + types)\n",
    "    bigramsa1[i] = bia1_mle[i]*vocab[i[0]]\n",
    "    diff[i] = bigrams[i] - bigramsa1[i]\n",
    "sorted_diff = sorted(diff.items(), key=itemgetter(1))[::-1]\n",
    "print \"Bigrams, the change in count, original count, effective count after add-one smoothing for top 10 drastic changes are:\"\n",
    "for k in range(10):\n",
    "    print sorted_diff[k][0], sorted_diff[k][1], bigrams[sorted_diff[k][0]], bigramsa1[sorted_diff[k][0]] \n",
    "\n",
    "#We can use bia1_mle instead of bi_mle in the probability function to compute probability of a sentence according to add \n",
    "#one smoothing on bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drastic change because we distribute the probability mass distribution to the bigrams that we have not seen. Also the change is maximum for the bigrams with highest frquency because maximum probability mass is taken from them in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences for 1-gram model are: -----\n",
      "[0.2827461607949413, 0.2770780856423174, 0.47368421052631593, 0.8402777777777777, 0.9780219780219781, -1.0, -0.21311475409836067, 2.2727272727272725, -1.2857142857142865, 2.666666666666667]\n",
      "Mean:0.5292373112344623\n",
      "\n",
      "Differences for 2-gram model are: -----\n",
      "[0.6722103502740872, 0.8231292517006803, 1.2890173410404624, 0.4020270270270272, 1.056338028169014, 0.75, 0.980952380952381, 1.3924050632911396, 2.275862068965517, 0.4102564102564106]\n",
      "Mean:1.005219792167672\n",
      "\n",
      "Differences for 3-gram model are: -----\n",
      "[0.8549446241301578, 1.1972972972972973, 1.404040404040404, 1.689873417721519, 0.06849315068493134, 3.6666666666666665, 2.2, 0.5, 3.0, 2.666666666666667]\n",
      "Mean:1.7247982227207643\n",
      "\n",
      "Differences for 4-gram model are: -----\n",
      "[0.9441242058316153, 1.4984939759036144, 1.5225225225225225, 2.5365853658536586, 1.0, 4.25, -9.0, 5.75, -11.0, 4.5]\n",
      "Mean:0.20017260701114098\n",
      "\n",
      "Differences for 5-gram model are: -----\n",
      "[0.9782248716674946, 1.6007604562737643, 2.085714285714286, 2.75, -10.0, 6.0]\n",
      "Mean:0.5691166022759241\n",
      "\n",
      "d:0.8057089070819927\n"
     ]
    }
   ],
   "source": [
    "def GTS(n):\n",
    "    ngram_dict = {}\n",
    "    for sent in sents:\n",
    "        ngram_arr_n = list(ngrams(sent.split(), n))\n",
    "        for i in ngram_arr_n:\n",
    "            try:\n",
    "                ngram_dict[i] += 1\n",
    "            except:\n",
    "                ngram_dict[i] = 1 \n",
    "    m = max(ngram_dict.values())\n",
    "    freqList = [0]*(m+1)\n",
    "    for j in ngram_dict.keys():\n",
    "        c = ngram_dict[j]\n",
    "        freqList[c] +=1\n",
    "    freqList[0] = sum(ngram_dict.values())\n",
    "    \n",
    "    #Taking top 10 counts\n",
    "    print \"Differences for \" + str(n) + \"-gram model are: -----\"\n",
    "    diff = []\n",
    "    for i in range(1, min(m,11)):\n",
    "        if freqList[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            diff.append(i - ((i + 1.0)*freqList[i+1])/freqList[i])\n",
    "    print diff\n",
    "    mean = np.mean(diff)\n",
    "    print \"Mean:\" + str(mean) + \"\\n\"\n",
    "    return mean\n",
    "\n",
    "mean = []\n",
    "for i in range(1,6):\n",
    "    m = GTS(i)\n",
    "    mean.append(m)\n",
    "print \"d:\" +str(np.mean(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D values fluctuates for different models. However it is somewhat constant in a single model for different counts. Also, it is sometomes negative and sometimes positive. Mean d:0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of add one smoothing and good turing smoothing for bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of add one smoothing: 570.2244425739852\n",
      "Perplexity of good turing smoothing: 32.66473807037255\n",
      "Good turing is better by 17.45688091377007 times\n"
     ]
    }
   ],
   "source": [
    "#vocab for train\n",
    "vocab_train = {}\n",
    "for sent in train:\n",
    "    ssent = sent.split()\n",
    "    for i in ssent:\n",
    "        try:\n",
    "            vocab_train[i]+=1\n",
    "        except KeyError:\n",
    "            vocab_train[i] = 1 \n",
    "vocab_types = len(vocab_train.keys())\n",
    "\n",
    "#bigrams for train set\n",
    "bigrams_train = {}\n",
    "bi_mle_train = {}\n",
    "for sent in train:\n",
    "    ssent = sent.split()\n",
    "    for i in range(0,len(ssent)-1):\n",
    "        k = (ssent[i], ssent[i+1])\n",
    "        try:\n",
    "            bigrams_train[k] += 1\n",
    "        except KeyError:\n",
    "            bigrams_train[k] = 1\n",
    "for i in bigrams_train.keys():\n",
    "    bi_mle_train[i] = (bigrams_train[i]*1.0)/vocab_train[i[0]]\n",
    "\n",
    "#add-one soothing on trainset\n",
    "bia1_mle_train = {}\n",
    "for i in bigrams_train.keys():\n",
    "    bia1_mle_train[i] = (bigrams_train[i] + 1.0)/(vocab_train[i[0]] + vocab_types)\n",
    "\n",
    "#calculating perplexity of add one smoothing\n",
    "a1p = []\n",
    "for j in test:\n",
    "    p=0\n",
    "    ssent = j.split()\n",
    "    for i in range(1, len(ssent)):\n",
    "        try:\n",
    "            p = p + log(1.0/bia1_mle_train[(ssent[i-1], ssent[i])])\n",
    "        except KeyError:\n",
    "            try:\n",
    "                p = p + log((vocab_train[ssent[i-1]] + vocab_types))    #bia1_mle_train['<unk>'] = 1.0/(vocab_train[ssent[i-1]] + vocab_types)\n",
    "            except KeyError:\n",
    "                p = p + log(1+vocab_types)              #vocab_train[ssent[i-1]] = 1\n",
    "    a1p.append(exp(p*(1.0/(len(ssent)))))\n",
    "print \"Perplexity of add one smoothing: \" + str(np.mean(a1p))\n",
    "\n",
    "#calculating perplexity for good turing smoothing\n",
    "m = max(bigrams_train.values())\n",
    "freqList = [0]*(m+1)\n",
    "for j in bigrams_train.keys():\n",
    "    c = bigrams_train[j]\n",
    "    freqList[c] +=1\n",
    "freqList[0] = sum(bigrams_train.values())\n",
    "\n",
    "#We use the value of d from the previous part to calculate the effective counts. \n",
    "d = 0.8786209670155173 #d for bigram model\n",
    "bigts_mle_train = {}\n",
    "for i in bigrams_train.keys():\n",
    "    bigts_mle_train[i] = (bigrams_train[i] - d)/(vocab_train[i[0]])\n",
    "    \n",
    "#Number of unigrams with unit frequency \n",
    "n_uni = 0\n",
    "for i in vocab_train.values():\n",
    "    if i==1:\n",
    "        n_uni +=1\n",
    "    \n",
    "#calculating perplexity of good turing smoothing\n",
    "gtsp = []\n",
    "for j in test:\n",
    "    p=0\n",
    "    ssent = j.split()\n",
    "    for i in range(1, len(ssent)):\n",
    "        try:\n",
    "            p = p + log(1.0/bigts_mle_train[(ssent[i-1], ssent[i])])\n",
    "        except KeyError:\n",
    "            try:\n",
    "                p = p + log((freqList[0]*vocab_train[ssent[i-1]])/freqList[1])    #bia1_mle_train['<unk>'] = c*/(vocab_train[ssent[i-1]])\n",
    "            except KeyError:\n",
    "                m = sum(vocab_train.values())\n",
    "                p = p + log((freqList[0]*n_uni*1.0)/(freqList[1]*m))    #vocab_train[ssent[i-1]] = n_uni/sum(vocab.values())\n",
    "    gtsp.append(exp(p*(1.0/(len(ssent)))))\n",
    "print \"Perplexity of good turing smoothing: \" + str(np.mean(gtsp))\n",
    "print \"Good turing is better by \" + str(np.mean(a1p)/np.mean(gtsp)) + ' times'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of good turing smoothing is less than the perplexity of add one smoothing. Thus, it is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
